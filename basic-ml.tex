% machine learning


%%%%%% ml - data
\newcommand{\Xspace}{\mathcal{X}}                                           % X, input space
\newcommand{\Yspace}{\mathcal{Y}}                                           % Y, output space
\newcommand{\nset}{\{1, \ldots, n\}}                                        % set from 1 to n
\newcommand{\pset}{\{1, \ldots, p\}}                                        % set from 1 to p
\newcommand{\gset}{\{1, \ldots, g\}}                                        % set from 1 to g
\newcommand{\Pxy}{\P_{xy}}                                                  % P_xy
\newcommand{\Exy}{\mathbb{E}_{xy}}                                          % E_xy: Expectation over random variables xy
\newcommand{\xy}{(\xv, y)}                                                  % observation (x, y)
\newcommand{\xvec}{\left(x_1, \ldots, x_p\right)^T}                         % (x1, ..., xp) 
\newcommand{\Xmat}{\mathbf{X}}											  % Design matrix
\newcommand{\allDatasets}{\mathds{D}}                                       % The set of all datasets
\newcommand{\D}{\mathcal{D}}                                                      % D, data
\newcommand{\obs}[1][i]{\left(\xv^{(#1)},y^{(#1)}\right)}                                                                               % observation (x^(i), y^(i))
\newcommand{\Dset}{\left( \obs[1], \ldots, \obs[n]\right)}    % {(x1,y1)), ..., (xn,yn)}, data
\newcommand{\Dn}{\D_n}                                                      % D_n, data of size n
\newcommand{\allDatasetsn}{\mathds{D}_n}                                    % The set of all datasets of size n 
\newcommand{\defAllDatasetsn}{(\Xspace \times \Yspace)^n}                   % Def. of the set of all datasets of size n 
\newcommand{\defAllDatasets}{\bigcup_{n \in \N}(\Xspace \times \Yspace)^n}  % Def. of the set of all datasets 
\newcommand{\xdat}{\left\{ \xv^{(1)}, \ldots, \xv^{(n)}\right\}}   				% {x1, ..., xn}, input data
\newcommand{\ydat}{\mathbf{y}}                                              % y (bold), vector of outcomes
\newcommand{\yvec}{\left(y^{(1)}, \hdots, y^{(n)}\right)^T}                 % (y1, ..., yn), vector of outcomes
\renewcommand{\xi}[1][i]{\xv^{(#1)}}                                          % x^i, i-th observed value of x
\newcommand{\yi}[1][i]{y^{(#1)}}                                            % y^i, i-th observed value of y 
\newcommand{\xyi}[1][i]{\left(\xv^{(#1)}, y^{(#1)}\right)}                                    % (x^i, y^i), i-th observation
\newcommand{\xivec}{\left(x^{(i)}_1, \ldots, x^{(i)}_p\right)^T}            % (x1^i, ..., xp^i), i-th observation vector
\newcommand{\xj}{\xv_j}                                                       % x_j, j-th feature
\newcommand{\xjvec}{\left(x^{(1)}_j, \ldots, x^{(n)}_j\right)^T}            % (x^1_j, ..., x^n_j), j-th feature vector
\newcommand{\Dtrain}{\mathcal{D}_{\text{train}}}                            % D_train, training set
\newcommand{\Dtest}{\mathcal{D}_{\text{test}}}                              % D_test, test set
\newcommand{\phiv}{\mathbf{\phi}}												% Basis transformation function phi
\newcommand{\phixi}{\mathbf{\phi}^{(i)}}										% Basis transformation of xi: phi^i := phi(xi)

%%%%%% ml - models general
\newcommand{\lamv}{\bm{\lambda}}											% lambda vector, hyperconfiguration vector
\newcommand{\Lam}{\bm{\Lambda}}											% Lambda, space of all hpos
% Inducer / Inducing algorithm
\newcommand{\preimageInducer}{\left(\defAllDatasets\right)\times\Lam}     % Set of all datasets times the hyperparameter space
\newcommand{\preimageInducerShort}{\allDatasets\times\Lam}     % Set of all datasets times the hyperparameter space
% Inducer / Inducing algorithm
\newcommand{\inducer}{\mathcal{I}}                                                % Inducer, inducing algorithm, learning algorithm 

% continuous prediction function f
\newcommand{\ftrue}{f_{\text{true}}}										  % True underlying function (if a statistical model is assumed)
\newcommand{\ftruex}{\ftrue(\xv)}										  % True underlying function (if a statistical model is assumed)
\newcommand{\fx}{f(\xv)}                                                      % f(x), continuous prediction function
\newcommand{\Hspace}{\mathcal{H}}														% hypothesis space where f is from
\newcommand{\fix}{f_i(\xv)}                                                      % f_i(x), discriminant component function
\newcommand{\fjx}{f_j(\xv)}                                                      % f_j(x), discriminant component function
\newcommand{\fkx}{f_k(\xv)}                                                      % f_k(x), discriminant component function
\newcommand{\fgx}{f_g(\xv)}                                                      % f_g(x), discriminant component function
\newcommand{\fh}{\hat{f}}                                                   % f hat, estimated prediction function
\newcommand{\fxh}{\fh(\xv)}                                                   % fhat(x)
\newcommand{\fxt}{f(\xv ~|~ \bm{\theta})}                                            % f(x | theta)
\newcommand{\fxi}{f\left(\xv^{(i)}\right)}                                        % f(x^(i))
\newcommand{\fxih}{\hat{f}\left(\xv^{(i)}\right)}                                 % f(x^(i))
\newcommand{\fxit}{f\left(\xv^{(i)} ~|~ \bm{\theta}\right)}                          % f(x^(i) | theta)
\newcommand{\fhD}{\fh_{\D}}                                                 % fhat_D, estimate of f based on D
\newcommand{\fhDtrain}{\fh_{\Dtrain}}                                       % fhat_Dtrain, estimate of f based on D
\newcommand{\fhDnlambda}{\fh_{\Dn, \lamv}}                                       %model learned on Dn with hp lambda
\newcommand{\fhDlambda}{\fh_{\D, \lamv}}                                       %model learned on D with hp lambda
\newcommand{\fhDnlambdastar}{\fh_{\Dn, \lamv^\ast}}                                       %model learned on Dn with optimal hp lambda 
\newcommand{\fhDlambdastar}{\fh_{\D, \lamv^\ast}}                                       %model learned on D with optimal hp lambda 

% discrete prediction function h
\newcommand{\hx}{h(\xv)}                                                      % h(x), discrete prediction function
\newcommand{\hxv}{h(\xv)}                                                      % h(x), discrete prediction function with x (vector) as input
\newcommand{\hh}{\hat{h}}                                                   % h hat
\newcommand{\hxh}{\hat{h}(\xv)}                                               % hhat(x)
\newcommand{\hxt}{h(\xv | \bm{\theta})}                                            % h(x | theta)
\newcommand{\hxi}{h\left(\xi\right)}                                        % h(x^(i))
\newcommand{\hxit}{h\left(\xi ~|~ \bm{\theta}\right)}                          % h(x^(i) | theta)

% yhat
\newcommand{\yh}{\hat{y}}                                                   % yhat for prediction of target
\newcommand{\yih}{\hat{y}^{(i)}}                                            % yhat^(i) for prediction of ith targiet

% theta
\newcommand{\thetah}{\bm{\hat{\theta}}}  
\newcommand{\thetab}{\bm{\theta}}											% theta vector
\newcommand{\thetabh}{\bm{\hat\theta}}											% theta vector
\newcommand{\thetat}{\bm{\theta}^{[t]}}											% theta^[t] in optimization
\newcommand{\thetatn}{\bm{\theta}^{[t+1]}}					        			% theta^[t+1] in optimization
\newcommand{\thetahDnlambda}{\thetah_{\Dn, \lamv}}             %theta learned on Dn with hp lambda
\newcommand{\thetahDlambda}{\thetah_{\D, \lamv}}             %theta learned on D with hp lambda
% densities + probabilities
% pdf of x 
\newcommand{\pdf}{p}                                                        % p
\newcommand{\pdfx}{p(\xv)}                                                    % p(x)
\newcommand{\pixt}{\pi(\xv~|~ \bm{\theta})}                                         % pi(x|theta), pdf of x given theta
\newcommand{\pixit}{\pi\left(\xi ~|~ \bm{\theta}\right)}                           % pi(x^i|theta), pdf of x given theta
\newcommand{\pixii}{\pi\left(\xi\right)}                           % pi(x^i), pdf of i-th x 

% pdf of (x, y)
\newcommand{\pdfxy}{p(\xv,y)}                                                 % p(x, y)
\newcommand{\pdfxyt}{p(\xv, y ~|~ \bm{\theta})}                                      % p(x, y | theta)
\newcommand{\pdfxyit}{p\left(\xi, \yi ~|~ \bm{\theta}\right)}                      % p(x^(i), y^(i) | theta)

% pdf of x given y
\newcommand{\pdfxyk}{p(\xv | y=k)}                                            % p(x | y = k)
\newcommand{\pdfxyj}{p(\xv | y=j)}                                            % p(x | y = j)
\newcommand{\lpdfxyk}{\log \pdfxyk}                                         % log p(x | y = k)
\newcommand{\pdfxiyk}{p\left(\xi | y=k\right)}                              % p(x^i | y = k)

% prior probabilities
\newcommand{\pik}{\pi_k}                                                    % pi_k, prior
\newcommand{\lpik}{\log \pik}                                               % log pi_k, log of the prior
\newcommand{\pit}{\pi(\bm{\theta})}												% Prior probability of parameter theta

% posterior probabilities
\newcommand{\post}{\P(y = 1 ~|~ \xv)}                                           % P(y = 1 | x), post. prob for y=1
\newcommand{\pix}{\pi(\xv)}                                                   % pi(x), P(y = 1 | x)
\newcommand{\postk}{\P(y = k ~|~ \xv)}                                          % P(y = k | y), post. prob for y=k
\newcommand{\pikx}{\pi_k(\xv)}                                                % pi_k(x), P(y = k | x)
\newcommand{\pikxt}{\pi_k(\xv ~|~ \bm{\theta})}                                      % pi_k(x | theta), P(y = k | x, theta)
\newcommand{\pijx}{\pi_j(\xv)}                                                % pi_j(x), P(y = j | x)
\newcommand{\pigx}{\pi_g(\xv)}                                                % pi_g(x), P(y = g | x)
\newcommand{\pdfygxt}{p(y ~|~\xv, \bm{\theta})}                                      % p(y | x, theta)
\newcommand{\pdfyigxit}{p\left(\yi ~|~\xi, \bm{\theta}\right)}                     % p(y^i |x^i, theta)
\newcommand{\lpdfygxt}{\log \pdfygxt }                                      % log p(y | x, theta)
\newcommand{\lpdfyigxit}{\log \pdfyigxit}                                   % log p(y^i |x^i, theta)
\newcommand{\pixh}{\hat \pi(\xv)}                                             % pi(x) hat, P(y = 1 | x) hat
\newcommand{\pikxh}{\hat \pi_k(\xv)}                                          % pi_k(x) hat, P(y = k | x) hat
\newcommand{\pixih}{\hat \pi(\xi)}                                      % pi(x^(i)) with hat
\newcommand{\pikxih}{\hat \pi_k(\xi)}                                   % pi_k(x^(i)) with hat

% residual and margin
\newcommand{\eps}{\epsilon}                                                 % residual, stochastic
\newcommand{\epsi}{\epsilon^{(i)}}                                          % epsilon^i, residual, stochastic
\newcommand{\epsh}{\hat{\epsilon}}                                          % residual, estimated
\newcommand{\yf}{y \fx}                                                     % y f(x), margin
\newcommand{\yfi}{\yi \fxi}                                                 % y^i f(x^i), margin
\newcommand{\Sigmah}{\hat \Sigma}											% estimated covariance matrix
\newcommand{\Sigmahj}{\hat \Sigma_j}										% estimated covariance matrix for the j-th class

% ml - loss, risk, likelihood
\newcommand{\Lyf}{L\left(y, f\right)}                                               % L(y, f), loss function
\newcommand{\Lxy}{L\left(y, \fx\right)}                                               % L(y, f(x)), loss function
\newcommand{\Lxyi}{L\left(\yi, \fxi\right)}                                 % L(y^i, f(x^i))
\newcommand{\Lxyt}{L\left(y, \fxt\right)}                                   % L(y, f(x | theta))
\newcommand{\Lxyit}{L\left(\yi, \fxit\right)}                               % L(y^i, f(x^i | theta)
\newcommand{\Lxym}{L\left(\yi, f\left(\bm{\tilde{x}}^{(i)} ~|~ \bm{\theta}\right)\right)}                      % L(y^i, f(tilde(x)^i | theta), 
\newcommand{\Lpixy}{L\left(y, \pix\right)}                                  % L(y, pi(x)), loss function
\newcommand{\Lpixyi}{L\left(\yi, \pixii\right)}                             % L(y^i, pi(x^i))
\newcommand{\Lpixyt}{L\left(y, \pixt\right)}                                  % L(y, pi(x | theta))
\newcommand{\Lpixyit}{L\left(\yi, \pixit\right)}                              % L(y^i, pi(x^i | theta)

\newcommand{\Lhxy}{L\left(y, \hx\right)}                                               % L(y, h(x)), loss function on discrete classes
\newcommand{\Lr}{L\left(r\right)}                                               % L(r), loss function defined on the residual (regression) / margin (classification)

                                                                            % a somewhat weird symbol, loss of the ith obs in a MINIBATCH
\newcommand{\risk}{\mathcal{R}}                                             % R, risk
\newcommand{\riskf}{\risk(f)}                                               % R(f), risk
\newcommand{\riskt}{\mathcal{R}(\bm{\theta})}                                    % R(theta), risk
\newcommand{\riske}{\mathcal{R}_{\text{emp}}}                               % R_emp, empirical risk (without factor 1 / n
\newcommand{\riskeb}{\bar{\mathcal{R}}_{\text{emp}}}                          % R_emp, empirical risk with factor 1 / n
\newcommand{\riskef}{\riske(f)}                                             % R_emp(f)
\newcommand{\risket}{\mathcal{R}_{\text{emp}}(\bm{\theta})}                      % R_emp(theta)
\newcommand{\riskr}{\mathcal{R}_{\text{reg}}}                               % R_reg, regularized risk
\newcommand{\riskrt}{\mathcal{R}_{\text{reg}}(\bm{\theta})}                      % R_reg(theta)
\newcommand{\riskrf}{\riskr(f)}                                             % R_reg(f)
\newcommand{\riskrth}{\hat{\mathcal{R}}_{\text{reg}}(\bm{\theta})}              % hat R_reg(theta)
\newcommand{\risketh}{\hat{\mathcal{R}}_{\text{emp}}(\bm{\theta})}			  % hat R_emp(theta)
\newcommand{\LL}{\mathcal{L}}                                               % L, likelihood
\newcommand{\LLt}{\mathcal{L}(\bm{\theta})}                                      % L(theta), likelihood
\newcommand{\logl}{\ell}                                                    % l, log-likelihood
\newcommand{\loglt}{\logl(\bm{\theta})}                                             % l(theta), log-likelihood
\newcommand{\LS}{\mathfrak{L}}                                              % ????????????
\newcommand{\TS}{\mathfrak{T}}                                              % ??????????????
\newcommand{\errtrain}{\text{err}_{\text{train}}}                           % training error
\newcommand{\errtest}{\text{err}_{\text{test}}}                             % training error
\newcommand{\errexp}{\overline{\text{err}_{\text{test}}}}                   % training error
